{
  
    
        "post0": {
            "title": "Awesome Github Actions",
            "content": "Deploy Github Pages with Github Actions . Just weeks ago, I set up this blog with Jekyll on Github Pages. However, I switched to fastpages because of Jupyter Notebook support. The swithcing isn’t that difficult, but something caught my attention during the process, and that is Github Actions. We have talked about the idea that only pushing source files to the Github repo and let Github Pages generate static files by its own. But Github Pages only supports Jekyll and limited number of plugins, and that is where Github Actions can help. . Basically, Github Actions is like a remote server that can execute some predifined commands. To use Github Actions to deploy our Github Pages, we still need to generate static files in our repo, but the generation can be done automatically by Github Actions, and generated files can be hosted in another branch other than master. Therefore, we can host all source files in the master branch, and every time we push commits to the master branch (e.g. new posts or modifications of layouts), the predifined Github Actions workflow is triggered and it will automatically generate all necessary static files in another branch. We just need to change the repo’s setting that decides which branch Github Pages site is built from. . Take my homepage repo as an example. I build it using React and Webpack from scratch. Webpack will generate static files in a folder called dist. This dist folder does not have to be in the master branch, but all files inside it should be the content of another branch called gh-pages, and the Github Pages is built from this branch. . There are some Github Actions in the marketplace to help us achieve that for Github Pages. I chose the one that fastpages uses, which is peaceiris/actions-gh-pages. The usage is actually pretty straightforward, the entire workflow is as follows: . name: build on: push: branches: - master jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 - name: Setup Node uses: actions/setup-node@v2.1.0 with: node-version: &#39;12.x&#39; - name: Cache dependencies uses: actions/cache@v2 with: path: ~/.npm key: ${{ runner.os }}-node-${{ hashFiles(&#39;**/package-lock.json&#39;) }} restore-keys: | ${{ runner.os }}-node- - run: npm ci - run: npm run build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./dist . At first, we give a name to the workflow, which is build. Then, we specify what will trigger this workflow: . on: push: branches: - master . That is, every time we push commits to the master branch, this workflow will be executed. Now, we define all commands in this workflow. We first specify the operating system, checkout the repo and setup Node.js of the version we want. The following action actions/cache@v2 is to fetch all cached dependencies using key, or create the cache of all dependencies if the cache does not exist, so that the workflow does not need to download all dependencies again next time. In Linux, npm downloads all packages to ~/.npm, which is the folder we need to cache. The key of the cache is named after the OS and the hashing result of package-lock.json, which is reasonable since these two will decide the packages that npm downloads. . Now, we run bash command npm ci to download all dependencies (if one dependency exists in cache, npm will just use the alreay downloaded package, while new dependencies will be downloaded from the Internet). Then, we run npm run build to generate all static files. In my package.json, npm run build will use Webpack to achieve that: . { ... &quot;scripts&quot;: { &quot;start&quot;: &quot;webpack-dev-server --mode development&quot;, &quot;build&quot;: &quot;webpack --mode production&quot; } ... } . Webpack generate all files in the /dist folder, and we should deploy all content in this folder to the branch gh-pages. That is excatly what peaceiris/actions-gh-pages does at last. As for ${{ secrets.GITHUB_TOKEN }}, this is an automactically generated token by Github for authentication in the workflow, since this action needs to be authorized to write into our repository. We will find more usages of secrets in the next part. . Scheduled Spotify Playlists Updating with Github Actions . I once wrote a Python script that can update my Spotify top 200 playlists with data from SpotifyCharts. To update playlists daily, this script will be executed everyday on my own server. However, with Github Actions, we can schedule a workflow that runs the script at any time. The repo is at graysonliu/spotify-top-200-playlist-generator. We use this workflow to update the playlists daily. At the beginning, apart from push-triggering, this workflow is also triggered at 00:00, 06:00, 12:00 and 18:00 every day: . on: push: branches: - master schedule: # * is a special character in YAML so you have to quote this string - cron: &#39;0 0,6,12,18 * * *&#39; . Similar to the previous Node.js project, we also need to checkout the repo, specify the version of operating system and Python, cache and install all dependencies that pip downloads: . - uses: actions/checkout@v2 - name: Set up Python 3.6 uses: actions/setup-python@v2 with: python-version: 3.6 - uses: actions/cache@v2 with: path: ~/.cache/pip key: ${{ runner.os }}-pip-${{ hashFiles(&#39;**/requirements.txt&#39;) }} restore-keys: | ${{ runner.os }}-pip- - name: Install dependencies run: | python -m pip install --upgrade pip if [ -f requirements.txt ]; then pip install -r requirements.txt; fi . Then, we execute the Python script to update our playlists. When we run this script locally, we can set client id and client secret of our Spotify app as environment variables in .env, then we can get their values in Python script by simply reading environment variables. However, sensitive information like client id and client secret should not be exposed. Therefore, we add them as secrets at Github, and set them as environment variables in the workflow (actually, client id is not sensitive at all, you can just put it in the config.yml). . Modifying playlists in Spotify needs authentication, and that requires a browser. However, since Github Actions works in a headless environment, it is impossible to use a browser for authentication. Our strategy is, we first authenticate the app locally, which will give us a .cache file that saves tokens and can be used for authentication for a long period of time. We create secret AUTH_CACHE that saves the content of .cache at Github and also set it as a environment variable in the runtime. In the workflow, when we execute the Python script, we fetch this environment variable and create a .cache file using this secret. This newly created .cache file will be used for authentication. . I tried to create .cache by echo command in the workflow directly: . run: echo ${{ secrets.SPOTIFY_AUTH_CACHE }} &gt;&gt; .cache . This will not work because AUTH_CACHE is a JSON string that has quotation marks inside it, but echo somehow omits those quotation marks. So, we as well use Python to achieve that: . auth_cache = os.getenv(&#39;AUTH_CACHE&#39;) if auth_cache: with open(&#39;.cache&#39;, &#39;w&#39;) as f: f.write(auth_cache) . Another thing is, tokens in .cache could be refreshed in the runtime. If we do not update AUTH_CACHE to keep up with the content of .cache, tokens saved in AUTH_CACHE could be expired. Therefore, after the playlists are updated, we have to write the content of .cache back to secret AUTH_CACHE in case file .cache changes (though it seems it never changes according to my experience). We have to use Github APIs to update secrets in a workflow, but that needs authentication from Github. The default ${{ secrets.GITHUB_TOKEN }} does not have the permission to write secrets. For that purpose, we need a token with special scopes of permissions. First, we create a personal access token named TOKEN_WRITE_SECRETS with scopes as follows: . . Copy the generated token, and add it to secrets. We also name this secret TOKEN_WRITE_SECRETS. . In total, we should have four secrets now: . . After the updating is finished, we write the content of .cache back to AUTH_CACHE in the Python script. To write a secret, we should encrypt its content using the repo’s public key first. This public key can also be acquired by Github APIs (see this): . # update secret AUTH_CACHE in case that content in .cache is changed during runtime (e.g. token is refreshed) secret_name = &#39;AUTH_CACHE&#39; # we need authentication to write secrets # default GITHUB_TOKEN does not have the permission to write secrets, we need to create a personal access token # we also need to set this personal access token as an environment variable token_write_secrets = os.getenv(&#39;TOKEN_WRITE_SECRETS&#39;) # these are default environment variables in Github Actions github_api_url = os.getenv(&#39;GITHUB_API_URL&#39;) github_repo = os.getenv(&#39;GITHUB_REPOSITORY&#39;) github_actor = os.getenv(&#39;GITHUB_ACTOR&#39;) # for authentication from requests.auth import HTTPBasicAuth auth = HTTPBasicAuth(github_actor, token_write_secrets) headers = {&#39;Accept&#39;: &#39;application/vnd.github.v3+json&#39;} # Get the public key to encrypt secrets # reference: https://docs.github.com/en/free-pro-team@latest/rest/reference/actions#get-a-repository-public-key r = requests.get(f&#39;{github_api_url}/repos/{github_repo}/actions/secrets/public-key&#39;, headers=headers, auth=auth) public_key = r.json()[&#39;key&#39;] key_id = r.json()[&#39;key_id&#39;] . Then, we invoke the Github API that modifies secrets (see this): . # reference: https://docs.github.com/en/free-pro-team@latest/rest/reference/actions#create-or-update-a-repository-secret from base64 import b64encode from nacl import encoding, public def encrypt(public_key: str, secret_value: str) -&gt; str: &quot;&quot;&quot;Encrypt a Unicode string using the public key.&quot;&quot;&quot; public_key = public.PublicKey(public_key.encode(&quot;utf-8&quot;), encoding.Base64Encoder()) sealed_box = public.SealedBox(public_key) encrypted = sealed_box.encrypt(secret_value.encode(&quot;utf-8&quot;)) return b64encode(encrypted).decode(&quot;utf-8&quot;) with open(&#39;.cache&#39;, &#39;r&#39;) as f: encrypted_value = encrypt(public_key, f.read()) data = {&#39;encrypted_value&#39;: encrypted_value, &#39;key_id&#39;: key_id} r = requests.put(f&#39;{github_api_url}/repos/{github_repo}/actions/secrets/{secret_name}&#39;, headers=headers, json=data, auth=auth) if r.ok: print(f&#39;Secret {secret_name} updated.&#39;) . Run the entire Python script in the workflow with four secrets we defined before: . - name: Set environment variables with secrets and update playlists env: CLIENT_ID: ${{ secrets.CLIENT_ID }} CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }} AUTH_CACHE: ${{ secrets.AUTH_CACHE }} TOKEN_WRITE_SECRETS: ${{ secrets.TOKEN_WRITE_SECRETS }} run: python generate_top_200_playlist.py . Now, this workflow will update Spotify playlists at scheduled times every day. It uses AUTH_CACHE for authentication for Spotify, and uses a personal access token TOKEN_WRITE_SECRETS for authentication for Github APIs that help us to write the content of .cache back to secret AUTH_CACHE. . At last, we remove .cache since its content has already been saved in secret AUTH_CACHE: . - name: Delete local authorization cache file run: rm -f .cache . This is mainly for security reasons, though it might not be necessary since this file was created in the workflow and its lifecycle should end in the current workflow run. .",
            "url": "https://graysonliu.github.io/blog/github/node.js/python/2020/10/14/awesome-github-actions.html",
            "relUrl": "/github/node.js/python/2020/10/14/awesome-github-actions.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Lazy Evaluation & Late Binding: Unexpected Behavior of Iterables in Python",
            "content": "Glossary . iterable in Python: . An object capable of returning its members one at a time. . Anything that can be used in a for loop, such as list, tuple, str, dict, set and iterators. We can convert an iterable to an iterator, which is actually what for statement does, by passing it to the built-in function iter(). . iterator in Python:&gt; An object representing a stream of data. It must have the __next__() method (or can be passed to the built-in function next()), such as generators, map objects. . The Problem . Recently, I&#39;ve been practicing on Leetcode. In problem 373. Find K Pairs with Smallest Sums, I implemented a solution inspired by discussions in the community using heapq.merge in Python. . The main idea of heapq.merge is like a multi-way merge sort on many already-sorted iterables. The difference is that heapq.merge uses a heap for ordering. Heap is just a binary tree in Python, and it is implemented with an array. heapq.merge initially collects the first element of each input iterable to form a list, then it heapify the list, transforming it to a heap. The first element of heap—h[0], is always the smallest in the heap (supposing it is a min-heap). heap.merge just pop h[0] and puts it into the result iterator, and then push the following element of h[0] from the same input iterable into the heap (more details at source code). We can also use heap as a priority queue. . The implemention is as follows: . from typing import List import heapq, itertools def kSmallestPairs(nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]: # we have m*n pairs, imagine a m*n matrix rows = (([u, v] for v in nums2) for u in nums1) # since both nums1 and nums2 are ascending # each row is already sorted based on the sum of u and v # regular slicing cannot be applied to generator since &#39;generator&#39; object does not have __getitem__ # we should use itertools.islice return list(itertools.islice(heapq.merge(*rows, key=sum), k)) nums1 = [1, 7, 11] nums2 = [2, 4, 6] print(kSmallestPairs(nums1, nums2, k=9)) . [[11, 2], [11, 2], [11, 2], [11, 4], [11, 4], [11, 4], [11, 6], [11, 6], [11, 6]] . rows is a generator of generators. We expect k pairs of smallest sums as the output, which should be: . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . But the output is not what we expected. Is something wrong with the generator? Try following code: . rows = (([u, v] for v in nums2) for u in nums1) for row in rows: for element in row: print(element, end=&#39; &#39;) . [1, 2] [1, 4] [1, 6] [7, 2] [7, 4] [7, 6] [11, 2] [11, 4] [11, 6] . Surprisingly, the output is what we expect. . Lazy Evaluation . We know that generators in Python are lazy evaluation. An element in a generator is only evaluated when we iterate to it. Most other objects in Python are eager evaluation: . j = 1 L = [j for _ in range(4)] # list G = (j for _ in range(4)) # generator j = 2 print(L) print(list(G)) . [1, 1, 1, 1] [2, 2, 2, 2] . In above snippet, list L is evaluated when j=1, while generator G is not evaluated until we call list(G), at that time j=2. . Back to the generator rows, if we make it a list of generators instead of a generator of generators: . rows = [([u, v] for v in nums2) for u in nums1] for row in rows: for element in row: print(element, end=&#39; &#39;) . [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] . The output is different from when rows is a generator, but it is similar to the output of heapq.merge. The only difference is we did not sort the list as in heapq.merge. . If we think about the evaluation mechanism, when we create rows as a list, since list is eager evaluation, u should be 11 after that line is executed. That is why when we iterate those three generators later, the first elements of all pairs are 11. When rows is a generator, since generators are lazy evaluation, the value of u only changes after we finish iterating each row. . Back to heapq.merge, which is actually heapq.merge(*iterables, key=None, reverse=False). The first parameter iterables becomes a tuple after it receives input iterables. We can easily check this by following snippet: . rows = (([u, v] for v in nums2) for u in nums1) def func(*iterables): print(type(iterables)) for it in iterables: for pair in it: print(pair, end=&#39; &#39;) func(*rows) . &lt;class &#39;tuple&#39;&gt; [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] . The output is the same as when rows is a list even if we pass rows as a generator. Because it is converted to a tuple in the function, and tuple is also eager evaluation just like list. . However, even if heapq.merge does not convert rows to a tuple, we still cannot expect correct output. We mentioned before that heapq.merge initially collects the first element of each input iterable, considering following code: . rows = (([u, v] for v in nums2) for u in nums1) it1 = next(rows) print(next(it1)) # [1, 2] it2 = next(rows) print(next(it1)) # [7, 4], we expect [1, 4] print(next(it2)) # [7, 2] it3 = next(rows) print(next(it1)) # [11, 6], we expect [1, 6] print(next(it2)) # [11, 4], we expect [7, 4] print(next(it3)) # [11, 2] . [1, 2] [7, 4] [7, 2] [11, 6] [11, 4] [11, 2] . We find that every time we call next(rows), the value of u changes. Thus, even if heapq.merge dose not convert rows to a tuple, we can only guarantee correct pairs for the first element in each generator. After that, the first value of all subsequent pairs will be the same. . Solution . The correct way to solve the above problem is to make rows a map object: . rows = map(lambda u: ([u, v] for v in nums2), nums1) print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . This map object is also converted to a tuple when passing to heapq.merge, however, it gives us correct result. map objects are iterators, using lazy evaluation just like generators. The difference here is that this uses a function to create three generators. That u here is a parameter of the lambda function, not a single variable that holds the value of an element in nums1, and this function will be called three times with different values passed in. Therefore, this actually has nothing to do with map. It really depends on what u is. If we define rows as follows, we can also get the correct output: . rows = ((lambda u: ([u, v] for v in nums2))(k) for k in nums1) print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . rows = [(lambda u: ([u, v] for v in nums2))(k) for k in nums1] print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . We replaced u with k, which holds the value of the element currently being iterated in nums1, to make the structure more clear. However, this is not necessary since even if we use u to iterate nums1, in the lambda function, u will be overwritten by the parameter u of the function. . To better understand how this u affects the output, we can also use lambda expressions to write the equivalents of previous definitions of rows that give us incorrect results: . rows = (([u, v] for v in nums2) for u in nums1) # is equivalent to rows = ((lambda: ([u, v] for v in nums2))() for u in nums1) rows = [([u, v] for v in nums2) for u in nums1] # is equivalent to rows = [(lambda: ([u, v] for v in nums2))() for u in nums1] . Late Binding . I also want to mention late binding here because it usually leads to unexpected behavior in Python, though not specifically limited to iterables. According to Wikipedia: . Late binding, dynamic binding, or dynamic linkage is a computer programming mechanism in which the method being called upon an object or the function being called with arguments is looked up by name at runtime. . There is a good article talking about late binding in Python. We will use some snippets to figure out how late binding works. . funcs = [lambda: u for u in range(5)] for func in funcs: print(func(), end=&#39; &#39;) . 4 4 4 4 4 . The output again is a repeat of the last element of range(5). We defined funcs, which is a list of 5 functions. Each function does not accept any parameters and returns u, which holds the value of the element being iterated in range(5). These functions are not called until we enter the following for loop and use func(). . Late binding means that values of variables used in a function are looked up at the time the function is called. When we call those five functions in funcs, they will all look up the value of u, which becomes 4 after we define funcs, because funcs, which is a list, use eager evaluation. However, if we define funcs as generators, the output becomes what we expect: . funcs = (lambda: u for u in range(5)) for func in funcs: print(func(), end=&#39; &#39;) . 0 1 2 3 4 . This is obvious since generators use lazy evaluation. The value of u changes along the for loop. . Another example of late binding without iterables: . i = 1 def func(): return i i = 2 print(func()) . 2 .",
            "url": "https://graysonliu.github.io/blog/coding/python/iterator/lazy-evaluation/late-binding/leetcode/heap/2020/09/22/lazy-evaluation-late-binding-unexpected-behavior-of-iterables-in-python.html",
            "relUrl": "/coding/python/iterator/lazy-evaluation/late-binding/leetcode/heap/2020/09/22/lazy-evaluation-late-binding-unexpected-behavior-of-iterables-in-python.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Using a Forked Jekyll Theme for Github Pages",
            "content": "Switch to Jekyll . I have abandoned blogging for a very long time. I used to build my blog on Hexo and hosted it on Github Pages. A major drawback of this solution is that we have to first generate all static files locally, and then push all those files to the remote repository. As for Jekyll, since it is officially supported by Github Pages, we only need to push sources to the remote repository. Github Pages will generate the static site automatically, and those static files are not a part of our repository, making our repo much tidier. . There is a great guide on blogging with Github Pages and Jekyll, but this solution also has a pain point. Although Jekyll has plenty of plugins developed by the community, Github Pages only supports some specific ones, let alone most supported plugins are of an older version without newest features. If we want to beautify or add features to the blog, we have to modify corresponding templates by ourselves. Of course, we can add those modified templates to our Github Pages repo directly, but what if we want to use them as a separate theme like a Ruby gem. If this new theme is a fork of an existing theme, we might also want to update it from the upstream. Although Github Pages only supports specific Jekyll plugins, it could utilize any Jekyll theme that is hosted on Github. Therefore, we need to create our own Jekyll theme and host it on Github, then specify it as the the theme that should be used by Github Pages. . Configuration . I forked the minima theme and plan to develop my own theme based on it. Give the forked repo a new name, better to be the name you want for this new theme, e.g. minima-graysonliu. Then, in *.gemspec, simply change the value of spec.name to minima-graysonliu. We now have a Github-hosted Jekyll theme called minima-graysonliu. To use this theme, navigate to our Github Pages repo, in _config.yml, delete the following line: . theme: ... . and add a new line: . remote_theme: graysonliu/minima-graysonliu . The value of the remote_theme is just the name of your new theme repo hosted on Github, and we are done. . Other changes are needed if we want to modify and test our new theme locally. We first clone both the blog (Github Pages) repo and the theme repo to our local disk. In Gemfile of the blog repo, change . # This is the default theme for new Jekyll sites. You may change this to anything you like. gem &quot;minima&quot; . to . # This is the default theme for new Jekyll sites. You may change this to anything you like. gem &quot;minima-graysonliu&quot;, :path =&gt; &quot;/LOCAL_PATH_TO_THEME_REPO/minima-graysonliu&quot; . Update with: . bundle update . Even if _config.yml in the blog repo does not have the key theme (because we replaced it with remote_theme), since we specify minima-graysonliu as the default theme, Jekyll will use it when we run the server locally. . If we need to keep our forked new theme updated with the original theme, just set the original repo as its upstream and sync the fork regularly. .",
            "url": "https://graysonliu.github.io/blog/jekyll/blog/github/2020/09/21/using-a-forked-jekyll-theme-for-github-pages.html",
            "relUrl": "/jekyll/blog/github/2020/09/21/using-a-forked-jekyll-theme-for-github-pages.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://graysonliu.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://graysonliu.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://graysonliu.github.io/blog/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Visit my website. .",
          "url": "https://graysonliu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://graysonliu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}