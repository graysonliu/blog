{
  
    
        "post0": {
            "title": "LeetCode 37 - Sudoku Solver: Backtracking",
            "content": "The LeetCode problem Sudoku Solver is a classic backtracking problem. . According to wikipedia: . Backtracking incrementally builds candidates to the solutions, and abandons a candidate (&quot;backtracks&quot;) as soon as it determines that the candidate cannot possibly be completed to a valid solution. . Cannot possibly be completed to a valid solution means there are some constraints that the candidate cannot satisfy. Therefore, those kind of problems are often called constraint satisfaction problems. . For example, in a Sudoku problem, we have following constraints that must be satisfied:- Each of the digits 1-9 must occur exactly once in each row. . Each of the digits 1-9 must occur exactly once in each column. . | Each of the digits 1-9 must occur exactly once in each of the 9 3x3 sub-boxes of the grid. . | . To check those three constraints, we construct three corresponding two-dimensional list as follows: . rows_available_list = [[True] * 9 for i in range(9)] cols_available_list = [[True] * 9 for i in range(9)] sub_boxes_available_list = [[True] * 9 for i in range(9)] . Each value in rows_available_list[i] indicates whether a number has been used in this row. For example, if rows_available_list[i][j] is True, that means you can still fill an empty cell in row i+1 with number j+1 (index starts at 0), but if it is False, that means the number j+1 has already been used in this row. This is similar for cols_available_list. . For sub-boxes, the mapping relationship is a bit different. For cell board[i][j], we check numbers&#39; availability of this cell in sub_boxes_available_list[i // 3 * 3 + j // 3]. . So, every time we take a step towards the solution, we should check whether this step satisfies all constraints. Suppose in this step, we fill cell board[i][j] with number num, we need make sure this number is available in the row, colomn, and the sub-box that the cell belongs: . def check_availability(loc, num): i, j = loc index = num - 1 return rows_available_list[i][index] and cols_available_list[j][index] and sub_boxes_available_list[i // 3 * 3 + j // 3][index] . Also, we need to write a function that updates those three constraint lists every time a step is taken towards the solution: . def update_constraint_list(loc, num, set): &quot;&quot;&quot; loc: location of the cell set: True if the num is going to fill this cell, False if the num is to be removed from this cell &quot;&quot;&quot; i, j = loc index = num - 1 rows_available_list[i][index] = not set cols_available_list[j][index] = not set sub_boxes_available_list[i // 3 * 3 + j // 3][index] = not set . Of course, the constraint lists should have their initial values based on those already filled cells in the original board. By iterating through the original board, we can also figure out which cells are still empty. . board = [ [&quot;5&quot;, &quot;3&quot;, &quot;.&quot;, &quot;.&quot;, &quot;7&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;], [&quot;6&quot;, &quot;.&quot;, &quot;.&quot;, &quot;1&quot;, &quot;9&quot;, &quot;5&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;], [&quot;.&quot;, &quot;9&quot;, &quot;8&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;6&quot;, &quot;.&quot;], [&quot;8&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;6&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;3&quot;], [&quot;4&quot;, &quot;.&quot;, &quot;.&quot;, &quot;8&quot;, &quot;.&quot;, &quot;3&quot;, &quot;.&quot;, &quot;.&quot;, &quot;1&quot;], [&quot;7&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;2&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;6&quot;], [&quot;.&quot;, &quot;6&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;2&quot;, &quot;8&quot;, &quot;.&quot;], [&quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;4&quot;, &quot;1&quot;, &quot;9&quot;, &quot;.&quot;, &quot;.&quot;, &quot;5&quot;], [&quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;.&quot;, &quot;8&quot;, &quot;.&quot;, &quot;.&quot;, &quot;7&quot;, &quot;9&quot;] ] empty_cell_list = [] for i in range(9): for j in range(9): if board[i][j] != &#39;.&#39;: update_constraint_list((i, j), int(board[i][j]), True) else: empty_cell_list.append((i, j)) . Now, we are ready to write our solver with backtracking: . def solver(vacant_list, cur): if len(vacant_list) == cur: # we filled all empty cells, meaning we found a solution return True i, j = vacant_list[cur] for num in range(1, 10): if check_availability(vacant_list[cur], num): # take a step towards the solution board[i][j] = str(num) update_constraint_list((i, j), num, True) res = solver(vacant_list, cur + 1) # the next step if res: # we found a solution return True else: # we cannot find a solution after we take this step # backtracking &gt; take a step backward board[i][j] = &#39;.&#39; update_constraint_list((i, j), num, False) return False . Give it a test and check the output: . solver(empty_cell_list, 0) print(board) . [[&#39;5&#39;, &#39;3&#39;, &#39;4&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;1&#39;, &#39;2&#39;], [&#39;6&#39;, &#39;7&#39;, &#39;2&#39;, &#39;1&#39;, &#39;9&#39;, &#39;5&#39;, &#39;3&#39;, &#39;4&#39;, &#39;8&#39;], [&#39;1&#39;, &#39;9&#39;, &#39;8&#39;, &#39;3&#39;, &#39;4&#39;, &#39;2&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;], [&#39;8&#39;, &#39;5&#39;, &#39;9&#39;, &#39;7&#39;, &#39;6&#39;, &#39;1&#39;, &#39;4&#39;, &#39;2&#39;, &#39;3&#39;], [&#39;4&#39;, &#39;2&#39;, &#39;6&#39;, &#39;8&#39;, &#39;5&#39;, &#39;3&#39;, &#39;7&#39;, &#39;9&#39;, &#39;1&#39;], [&#39;7&#39;, &#39;1&#39;, &#39;3&#39;, &#39;9&#39;, &#39;2&#39;, &#39;4&#39;, &#39;8&#39;, &#39;5&#39;, &#39;6&#39;], [&#39;9&#39;, &#39;6&#39;, &#39;1&#39;, &#39;5&#39;, &#39;3&#39;, &#39;7&#39;, &#39;2&#39;, &#39;8&#39;, &#39;4&#39;], [&#39;2&#39;, &#39;8&#39;, &#39;7&#39;, &#39;4&#39;, &#39;1&#39;, &#39;9&#39;, &#39;6&#39;, &#39;3&#39;, &#39;5&#39;], [&#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;2&#39;, &#39;8&#39;, &#39;6&#39;, &#39;1&#39;, &#39;7&#39;, &#39;9&#39;]] .",
            "url": "https://graysonliu.github.io/blog/coding/python/leetcode/backtrack/dfs/2022/02/22/leetcode-37-sudoku-solver-backtrack.html",
            "relUrl": "/coding/python/leetcode/backtrack/dfs/2022/02/22/leetcode-37-sudoku-solver-backtrack.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Notes about multiprocessing in Python",
            "content": "GIL, threading, multiprocessing . GIL (Global Interpreter Lock) assures that only one thread executes Python bytecode in a process at a time. Therefore, multithreading program achieved by the threading module cannot fully utilize multi-core manchines. To write a parallel program in Python, the multiprocessing module should be used. . Since GIL is always released when doing I/O, threading is still an appropriate module if we want to run multiple I/O-bound tasks simultaneously. . Process . It is quite easy to create a new process in Python. We basically pass a target function and all arguments needed by the invocation to the Process object. Use start() to start the process&#39;s activity (i.e. the invocation of the target function). Use join() to block the parent process until the child process exits. . from multiprocessing import Process import time def worker(s): print(&#39;Start sleeping for 2 seconds...&#39;) time.sleep(2) print(&#39;Sleeping ends.&#39;) print(s) p = Process(target=worker, args=(&#39;Hello, world.&#39;,)) p.start() print(&#39;before join(), the parent process is not blocked&#39;) p.join() print(&#39;After join()&#39;) . Start sleeping for 2 seconds... before join(), the parent process is not blocked Sleeping ends. Hello, world. After join() . If an exception happens in the child process, the stack trace will also be printed in the console. . from multiprocessing import Process import time def worker(s): print(2 / 0) p = Process(target=worker, args=(&#39;Hello, world.&#39;,)) p.start() p.join() . Process Process-99: Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/process.py&#34;, line 315, in _bootstrap self.run() File &#34;/usr/lib/python3.8/multiprocessing/process.py&#34;, line 108, in run self._target(*self._args, **self._kwargs) File &#34;&lt;ipython-input-27-8d12741476dc&gt;&#34;, line 5, in worker print(2 / 0) ZeroDivisionError: division by zero . Process Pool . We can also create a pool of processes. The following snippet creates a process pool of size 4, and we put 8 tasks into the pool. . from multiprocessing import Pool import time import random def worker(i): time.sleep(random.random()) print(i) p = Pool(4) for i in range(8): p.apply_async(worker, args=(i,)) p.close() p.join() . 0 4 2 3 1 7 6 5 . When invoking apply_async(), a new task is submitted to the pool and the task will start once there is an empty slot in the pool. Pool also has a synchronized version of this method, apply(), which will block the parent process until the result is ready. Therefore, for parallelism, apply_async() should always be used. We also used join() method to block the parent process after we finished adding all tasks to the pool. If we do not block the parent process, parent process might exit before child processes finish their tasks. That&#39;s why we always use join() in the parent process to avoid this situation. Note that close() must be called before using join(). . Unlike Process, stack trace of exceptions in Pool will not be printed in the console, but we might need that when debugging a multiprocessing program. We can pass an error callback function to the apply_async() method, and that function will be used to print stack trace if exceptions happen. Note that the callback functions will block the parent process, so, do not put time-consuming tasks in the callback function since it should complete immediately. The error callback function only takes one argument and that is the exception instance. . from multiprocessing import Pool import traceback def worker(i): print(i / 0) def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) p = Pool(1) for i in range(2): p.apply_async(worker, args=(i,), error_callback=print_traceback) p.close() p.join() . None None multiprocessing.pool.RemoteTraceback: &#34;&#34;&#34; Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 125, in worker result = (True, func(*args, **kwds)) File &#34;&lt;ipython-input-50-90874a7b31e2&gt;&#34;, line 5, in worker print(i / 0) ZeroDivisionError: division by zero &#34;&#34;&#34; The above exception was the direct cause of the following exception: ZeroDivisionError: division by zero multiprocessing.pool.RemoteTraceback: &#34;&#34;&#34; Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 125, in worker result = (True, func(*args, **kwds)) File &#34;&lt;ipython-input-50-90874a7b31e2&gt;&#34;, line 5, in worker print(i / 0) ZeroDivisionError: division by zero &#34;&#34;&#34; The above exception was the direct cause of the following exception: ZeroDivisionError: division by zero . Communication Between Processes . I usually just use Queue for communication between processes, even if there are only two processes, in which Pipe also works. We don&#39;t have to worry about race conditions when using queue since queues are thread and process safe in Python. We will look at the one-producer-one-consumer case first. . from multiprocessing import Process, Queue import time, random def producer(q): for i in range(5): time.sleep(random.random()) q.put(i) q.put(&#39;end&#39;) # to notify the consumer process that the producer process has exited print(&#39;producer exits&#39;) def consumer(q): while True: product = q.get() if product == &#39;end&#39;: break print(product) print(&#39;consumer exits&#39;) q = Queue() p1 = Process(target=producer, args=(q,)) p2 = Process(target=consumer, args=(q,)) p1.start() p2.start() p1.join() p2.join() . 0 1 2 3 producer exits4 consumer exits . There might be race conditions on invocations of print(), so multiple lines could be shown in the same line. But other than that, everything should be expected. Note that the get() method of Queue will block the child process if no item in the queue is immediately available. We can avoid blocking by set argument block to Fasle or simply using get_nowait(), but it will raise an exception if it cannot get an item immediately. If we want to avoid blocking for too long, we can pass a timeout argument to get(). If the size of a queue is limited, the same approach can also be applied to the put() method. . We now look at the multi-producer-multi-consumer case, where we will use Pool. . from multiprocessing import Pool, Queue import time, random def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) def producer(q, producer_id): for i in range(5): time.sleep(random.random()) q.put(f&#39;product {i} from producer {producer_id}&#39;) q.put(&#39;end&#39;) # to notify the consumer process that the producer process has exited print(f&#39;producer {producer_id} exits&#39;) def consumer(q, consumer_id): while True: product = q.get() if product == &#39;end&#39;: break print(f&#39;{product} consumed by consumer {consumer_id}&#39;) print(f&#39;consumer {consumer_id} exits&#39;) q = Queue() producer_count = 2 consumer_count = 2 producer_pool = Pool(producer_count) consumer_pool = Pool(consumer_count) for i in range(producer_count): producer_pool.apply_async(producer, args=(q, i), error_callback=print_traceback) for i in range(consumer_count): consumer_pool.apply_async(consumer, args=(q, i), error_callback=print_traceback) producer_pool.close() consumer_pool.close() producer_pool.join() consumer_pool.join() . None None None None Traceback (most recent call last): Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 537, in _handle_tasks put(task) File &#34;/usr/lib/python3.8/multiprocessing/connection.py&#34;, line 206, in send self._send_bytes(_ForkingPickler.dumps(obj)) File &#34;/usr/lib/python3.8/multiprocessing/reduction.py&#34;, line 51, in dumps cls(buf, protocol).dump(obj) File &#34;/usr/lib/python3.8/multiprocessing/queues.py&#34;, line 58, in __getstate__ context.assert_spawning(self) File &#34;/usr/lib/python3.8/multiprocessing/context.py&#34;, line 359, in assert_spawning raise RuntimeError( File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 537, in _handle_tasks put(task) File &#34;/usr/lib/python3.8/multiprocessing/connection.py&#34;, line 206, in send self._send_bytes(_ForkingPickler.dumps(obj)) File &#34;/usr/lib/python3.8/multiprocessing/reduction.py&#34;, line 51, in dumps cls(buf, protocol).dump(obj) File &#34;/usr/lib/python3.8/multiprocessing/queues.py&#34;, line 58, in __getstate__ context.assert_spawning(self) File &#34;/usr/lib/python3.8/multiprocessing/context.py&#34;, line 359, in assert_spawning raise RuntimeError( RuntimeError: Queue objects should only be shared between processes through inheritance RuntimeError: Queue objects should only be shared between processes through inheritance Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 537, in _handle_tasks put(task) File &#34;/usr/lib/python3.8/multiprocessing/connection.py&#34;, line 206, in send self._send_bytes(_ForkingPickler.dumps(obj)) File &#34;/usr/lib/python3.8/multiprocessing/reduction.py&#34;, line 51, in dumps cls(buf, protocol).dump(obj) File &#34;/usr/lib/python3.8/multiprocessing/queues.py&#34;, line 58, in __getstate__ context.assert_spawning(self) File &#34;/usr/lib/python3.8/multiprocessing/context.py&#34;, line 359, in assert_spawning raise RuntimeError( RuntimeError: Queue objects should only be shared between processes through inheritance Traceback (most recent call last): File &#34;/usr/lib/python3.8/multiprocessing/pool.py&#34;, line 537, in _handle_tasks put(task) File &#34;/usr/lib/python3.8/multiprocessing/connection.py&#34;, line 206, in send self._send_bytes(_ForkingPickler.dumps(obj)) File &#34;/usr/lib/python3.8/multiprocessing/reduction.py&#34;, line 51, in dumps cls(buf, protocol).dump(obj) File &#34;/usr/lib/python3.8/multiprocessing/queues.py&#34;, line 58, in __getstate__ context.assert_spawning(self) File &#34;/usr/lib/python3.8/multiprocessing/context.py&#34;, line 359, in assert_spawning raise RuntimeError( RuntimeError: Queue objects should only be shared between processes through inheritance . We are getting RuntimeError, and it says Queue objects should only be shared between processes through inheritance. This happens if we try to use multiprocessing.Queue in a process pool. As for the solution, we should use multiprocessing.Manager.Queue among pool workers instead. . from multiprocessing import Pool, Manager import time, random def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) def producer(q, producer_id): for i in range(5): time.sleep(random.random()) q.put(f&#39;product {i} from producer {producer_id}&#39;) q.put(&#39;end&#39;) # to notify the consumer process that a producer process has exited print(f&#39;producer {producer_id} exits&#39;) def consumer(q, consumer_id): while True: product = q.get() if product == &#39;end&#39;: break print(f&#39;{product} consumed by consumer {consumer_id}&#39;) print(f&#39;consumer {consumer_id} exits&#39;) manager = Manager() q = manager.Queue() producer_count = 2 consumer_count = 2 producer_pool = Pool(producer_count) consumer_pool = Pool(consumer_count) for i in range(producer_count): producer_pool.apply_async(producer, args=(q, i), error_callback=print_traceback) for i in range(consumer_count): consumer_pool.apply_async(consumer, args=(q, i), error_callback=print_traceback) producer_pool.close() consumer_pool.close() producer_pool.join() consumer_pool.join() . product 0 from producer 1 consumed by consumer 0 product 0 from producer 0 consumed by consumer 1 product 1 from producer 1 consumed by consumer 0 product 1 from producer 0 consumed by consumer 1 product 2 from producer 0 consumed by consumer 0 product 2 from producer 1 consumed by consumer 1 product 3 from producer 0 consumed by consumer 0 product 3 from producer 1 consumed by consumer 1 consumer 1 exitsproduct 4 from producer 1 consumed by consumer 0 producer 1 exits product 4 from producer 0 consumed by consumer 0 producer 0 exitsconsumer 0 exits . Lock . The above snippet has several flaws. Firstly, invocations of print() in different processes has race conditions, which could mess up the printed text in the console. Also, a consumer exits immediately when it receives an end signal, which is not desired since there might be products from one producer enqueued after the end signal from another producer. We want consumers to exit only if all producers have exited. . To handle the print() problem, we need a lock to avoid race conditions. To know whether all producers have exited or not, we need a shared value among consumers, which counts the number of exited producers. This shared value will be increased by 1 every time a consumer receives an end signal. . from multiprocessing import Pool, Manager, Value import time, random def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) def producer(q, producer_id, print_lock): for i in range(5): time.sleep(random.random()) q.put(f&#39;product {i} from producer {producer_id}&#39;) q.put(&#39;end&#39;) # to notify the consumer process that a producer process has exited with print_lock: print(f&#39;producer {producer_id} exits&#39;) def consumer(q, consumer_id, producer_count, print_lock, exited_producer_count, count_lock): while True: try: product = q.get_nowait() except: if exited_producer_count.value == producer_count: # all producers have exited break else: continue if product == &#39;end&#39;: with count_lock: exited_producer_count.value += 1 continue with print_lock: print(f&#39;{product} consumed by consumer {consumer_id}&#39;) with print_lock: print(f&#39;consumer {consumer_id} exits&#39;) manager = Manager() q = manager.Queue() exited_producer_count = manager.Value(&#39;i&#39;, 0) # type code &#39;i&#39; means &#39;signed int&#39; in C type print_lock = manager.Lock() count_lock = manager.Lock() producer_count = 2 consumer_count = 2 producer_pool = Pool(producer_count) consumer_pool = Pool(consumer_count) for i in range(producer_count): producer_pool.apply_async(producer, args=(q, i, print_lock), error_callback=print_traceback) for i in range(consumer_count): consumer_pool.apply_async(consumer, args=(q, i, producer_count, print_lock, exited_producer_count, count_lock), error_callback=print_traceback) producer_pool.close() consumer_pool.close() producer_pool.join() consumer_pool.join() . product 0 from producer 1 consumed by consumer 0 product 0 from producer 0 consumed by consumer 0 product 1 from producer 1 consumed by consumer 0 product 2 from producer 1 consumed by consumer 1 product 1 from producer 0 consumed by consumer 0 product 3 from producer 1 consumed by consumer 0 product 2 from producer 0 consumed by consumer 0 product 4 from producer 1 consumed by consumer 0 producer 1 exits product 3 from producer 0 consumed by consumer 1 producer 0 exits product 4 from producer 0 consumed by consumer 0 consumer 1 exits consumer 0 exits . We defined two locks. The print_lock is acquired before each print() invocation. Another lock count_lock ensures that the increment of exited_producer_count is atomic. Same as Queue, we have to use multiprocessing.Manager.Value instead of multiprocessing.Value in a process pool. As shown in the output, each print() invocation takes one line exclusively, and consumers exited only if all producers have exited. . The following snipppet will demonstrate race conditions between processes. . from multiprocessing import Manager, Pool, Value def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) def add(counter): for _ in range(100): counter.value += 1 def sub(counter): for _ in range(100): counter.value -= 1 manager = Manager() counter = manager.Value(&#39;i&#39;, 0) add_pool = Pool(4) add_pool.starmap_async(add, [(counter,) for _ in range(4)], error_callback=print_traceback) sub_pool = Pool(2) sub_pool.starmap_async(sub, [(counter,) for _ in range(2)], error_callback=print_traceback) add_pool.close() sub_pool.close() add_pool.join() sub_pool.join() print(counter.value) . 49 . We have four add wokers and two sub wokers. Since each worker will change the shared value by +100/-100, we should have 200 as the final result. However, the result is not what we expected because race conditions happened. . We wrap the augmented assignment with a Manager.Lock in the following code, which gives us the correct output. Apart from with statements we used previously, we can also explictly call acquire() and release() method of Lock. . from multiprocessing import Manager, Pool, Value def print_traceback(e): print(traceback.print_exception(type(e), e, e.__traceback__)) def add(counter, counter_lock): for _ in range(100): counter_lock.acquire() counter.value += 1 counter_lock.release() def sub(counter, counter_lock): for _ in range(100): counter_lock.acquire() counter.value -= 1 counter_lock.release() manager = Manager() counter = manager.Value(&#39;i&#39;, 0) counter_lock = manager.Lock() add_pool = Pool(4) add_pool.starmap_async(add, [(counter, counter_lock) for _ in range(4)], error_callback=print_traceback) sub_pool = Pool(2) sub_pool.starmap_async(sub, [(counter, counter_lock) for _ in range(2)], error_callback=print_traceback) add_pool.close() sub_pool.close() add_pool.join() sub_pool.join() print(counter.value) . 200 .",
            "url": "https://graysonliu.github.io/blog/coding/python/parallelism/thread/process/2021/05/05/notes-about-multiprocessing-in-python.html",
            "relUrl": "/coding/python/parallelism/thread/process/2021/05/05/notes-about-multiprocessing-in-python.html",
            "date": " • May 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Docker Compose & Dockerfile with Node.js",
            "content": "",
            "url": "https://graysonliu.github.io/blog/node.js/docker/devops/2021/03/26/docker-compose-dockerfile-with-node-js.html",
            "relUrl": "/node.js/docker/devops/2021/03/26/docker-compose-dockerfile-with-node-js.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Python Formatting in VS Code: Force Whitespace around Arithmetic Operator",
            "content": "In VS Code, you can use Ctrl+Shift+I (Linux) to format a document by default shortcuts. For python, you need to install some formatter packages like autopep8 or black. . I use autopep8. Install it by pip install autopep8. By default, autopep8 won’t fixing missing whitespace around arithmetic operators. For example, if one line of your python code is a = 1+2, after using Ctrl+Shift+I, autopep8 won’t insert missing whitespace around the plus sign. If you want this line to be formatted into a = 1 + 2, you have to tweak the settings of autopep8. . According to the autopep8 page, four errors/warning are ignored by default: . --ignore errors . do not fix these errors/warnings (default:E226,E24,W50,W690) . And E226 is fix missing whitespace around arithmetic operator. Therefore, we have to bring E226 back. We can use --select options to tell autopep8 which errors and warnings should be fixed. In VS Code settings, search for autopep8, and add following two arguments: . . This means autopep8 will fix all code style errors and warnings, including previously ignored E226. Now, Ctrl+Shift+I will force whitespace around arithmetic operators. .",
            "url": "https://graysonliu.github.io/blog/python/vscode/coding/2021/03/18/python-formatting-in-vs-code.html",
            "relUrl": "/python/vscode/coding/2021/03/18/python-formatting-in-vs-code.html",
            "date": " • Mar 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "kubectl Commands",
            "content": "",
            "url": "https://graysonliu.github.io/blog/kubernetes/server/devops/infrastructure/docker/2021/03/04/kubectl-commands.html",
            "relUrl": "/kubernetes/server/devops/infrastructure/docker/2021/03/04/kubectl-commands.html",
            "date": " • Mar 4, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Intro to Prometheus & PromQL",
            "content": "This post is mainly based on documentations of Prometheus and an excellent introduction article in Chinese. . What is a Prometheus metric? . A Prometheus metric consists of a series of float64 values, with each value associated to a timestamp. Those timestamps in a metric form a time series. A time series is identified by the metric name and the combination of labels. A different metric name, or a different combination of labels, indicates a different time series. . For example, http_requests_total{method=&quot;GET&quot;, handler=&quot;/info&quot;} is a metric that records the total number of requests to endpoint /info with HTTP method GET received by the server, while http_requests_total{method=&quot;POST&quot;, handler=&quot;/info&quot;} cares about method POST for the same endpoint. Those two represents two different time series because they have different combination of labels, although the metric names are the same. . Metric Types . Counter .",
            "url": "https://graysonliu.github.io/blog/prometheus/server/devops/2021/02/23/intro-to-prometheus-promql.html",
            "relUrl": "/prometheus/server/devops/2021/02/23/intro-to-prometheus-promql.html",
            "date": " • Feb 23, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Set Up HTTPS for a Koa.js Server Using LetsEncrypt (Certbot)",
            "content": "The Koa backend of my small full-stack Node.js project has been upgraded to support HTTPS. Actually, since the frontend uses HTTPS, it cannot communicate with a server that only supports HTTP (browsers will block HTTP requests sent by a HTTPS page). I plan to use LetsEncrypt to acquire a free SSL certificate. Certbot, which LetsEncrypt recommends, has good support for popular servers like Apache and Nginx, but not with Node.js servers. You have to create the certificate manually for a Node.js server with certbot. A detailed article written by David Mellul talks about how to use certbot to generate the certificate for Express servers. This article closely follows his instructions, except that Express is changed to Koa. . Use Certbot to Create SSL Certificates . Create the certificate manually. . certbot certonly --manual . Enter your domain name: . Please enter in your domain name(s) (comma and/or space separated) (Enter &#39;c&#39; to cancel): &lt;your-domain-name&gt; . Give consent to IP logging. Then, the certbot will tell you what to do next on your server: . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a file containing just this data: &lt;data&gt; And make it available on your web server at this URL: http://&lt;your-domain-name&gt;/.well-known/acme-challenge/&lt;name&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - . Basically, you have to serve a static file on your server. The file is in directory .well-known/acme-challenge and it is named &lt;name&gt;. The content of the file should be &lt;data&gt;. . I use koa-static to server static files. Since my server is an API server, this certbot file is the only thing that my server will treat as static resources. I created a dedicated directory named letsencrypt to serve this static file. Put file &lt;name&gt; into directory ./letsencrypt/.well-known/acme-challenge, and use koa-static as follows: . const static = require(&#39;koa-static&#39;); app.use(static(&#39;./letsencrypt&#39;, {hidden: true})); // serve static files app.listen(80); . Note that .well-known is a hidden directory. We should set option hidden to true to serve hidden files/directories as static resources. Also, your server most likely does not use the default HTTP port 80. Make your server listen to port 80 for now. You can change it back to whatever you like after. . When all above have been done, visit that URL in your browser to check whether the setup is correct, and you should see the content &lt;data&gt; displayed. Then, back to your terminal and just follow the instructions that certbot gives you. . Use Certificates in the Koa Server . Certbot put created keys in /etc/letsencrypt/live/&lt;your-domain-name&gt; by default. In Koa, use the following code (I use port 3000): . const fs = require(&#39;fs&#39;); // SSL for HTTPS const options = { key: fs.readFileSync(&#39;/etc/letsencrypt/live/&lt;your-domain-name&gt;/privkey.pem&#39;), cert: fs.readFileSync(&#39;/etc/letsencrypt/live/&lt;your-domain-name&gt;/cert.pem&#39;), ca: fs.readFileSync(&#39;/etc/letsencrypt/live/&lt;your-domain-name&gt;/chain.pem&#39;) }; https.createServer(options, app.callback()).listen(3000); . Now, your server provides services using HTTPS. . Use cron and nodemon for Automatically Renewal . It should be noted that the certificates issued by LetsEncrypt will expire in 90 days. You have to renew certificates periodically. To renew manually created certificates, use . certbot renew --standalone . This command will only renew certificates that expires in 30 days. You can force the renewal by using . certbot renew --standalone --force-renewal . However, LetsEncrypt has rate limits. Forced renewal might fail if you exceed the limit. . I also use cron to schedule the renewal monthly. First, create a bash script file named ssl_renew.sh in your project directory. Make this file executable and write command certbot renew --standalone in it. Use crontab -e and append the following line in the opened editor: . 0 0 1 * * ./&lt;path-to-your-project&gt;/ssl_renew.sh . This basically renews certificates on the first day of every month. After the renewal, your server should be restarted to use the new certificates. This can also be done automatically. I use the tool nodemon to achieve that. nodemon can automatically restart the node application when file changes in directories are detected. To use nodemon, replace node with nodemon in package.json when executing your script: . &quot;scripts&quot;: { &quot;start&quot;: &quot;nodemon app.js&quot; } . Also, add configurations for nodemon in package.json: . &quot;nodemonConfig&quot;: { &quot;ignore&quot;: [ &quot;.git&quot;, &quot;test.js&quot; ], &quot;delay&quot;: &quot;2500&quot;, &quot;watch&quot;: [ &quot;.&quot;, &quot;/etc/letsencrypt/live/&lt;your-domain-name&gt;&quot; ], &quot;ext&quot;: &quot;js, json, pem&quot; } . There are two related attributes in this configuration. First, watch indicates directories or files where nodemon should monitor changes. Obviously, we should add /etc/letsencrypt/live/&lt;your-domain-name&gt;, where the certificates are saved. Also, by default, nodemon only detects changes in files with some specific extensions like .js and .json. In attribute ext, we should also add pem, which is the extension of certificates. Now, after the renewal, the Node.js server will automatically be restarted by nodemon. . Check my project for more details. .",
            "url": "https://graysonliu.github.io/blog/node.js/ssl/javascript/2021/01/12/set-up-https-for-a-koa-js-server-using-letsencrypt-certbot.html",
            "relUrl": "/node.js/ssl/javascript/2021/01/12/set-up-https-for-a-koa-js-server-using-letsencrypt-certbot.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Some Random Stuff on JavaScript",
            "content": "Inject Environment Variables into Web Pages at Build Time . Environment variables defined in .env can be accessed when webpack is generating web pages. After webpack finishes creating all those static files, the concept of environment variable does not exist anymore when we open a purely static web page in a browser. That means, at runtime, without support from a back end, we cannot get those environment variables that we used at build time. For a purely static web page, if we still want to have access to those environment variables at runtime in the browser, a circumvent is to inject those values into the web page at build time. If you are using Create React App, there is a simple way to achieve that according to its documentation: . &lt;title&gt;%REACT_APP_WEBSITE_NAME%&lt;/title&gt; . I am using webpack instead of Create React App. In that case, HtmlWebpackPlugin is the solution. Actually, there is a Github repo called jaketrent/html-webpack-template that has already showed the trick. . In the config file of webpack, we add options for HtmlWebpackPlugin as follows: . plugins: [ // ... new HtmlWebpackPlugin({ template: &quot;./src/template.ejs&quot;, // ... // inject environment variables into pages at build time window: { env: { env_var_1: process.env.ENV_VAR_1, env_var_2: process.env.ENV_VAR_2, env_var_3: process.env.ENV_VAR_3, } } // ... }), // ... ] . Then, in the template (note that we use suffix .ejs instead of .html for the template so that webpack can pick the correct loader), we add snippet in the &lt;head&gt;: . &lt;head&gt; &lt;!-- ... --&gt; &lt;!-- inject environment variables into pages at build time --&gt; &lt;script type=&quot;text/javascript&quot;&gt; &lt;% for (key in htmlWebpackPlugin.options.window) { %&gt; window[&#39;&lt;%= key %&gt;&#39;] = &lt;%= JSON.stringify(htmlWebpackPlugin.options.window[key]) %&gt;; &lt;% } %&gt; &lt;/script&gt; &lt;!-- ... --&gt; &lt;/head&gt; . Basically, we inject all environment variables into object window.env, which we have access to in the browser. In jaketrent/html-webpack-template, there are also many other notable tricks, like injecting Google Analytics info into the web page using HtmlWebpackPlugin. . Add/Omit a Property of a JavaScript Object Conditionally in One Line . When creating an object in JavaScript, if whether a property should be in this object is based on some condition, we can create this object utilizing spread syntax as follows: . let name = null; let person = { ...(name &amp;&amp; {name: name}) }; console.log(person); // {} name = &#39;Peter&#39;; person = { ...(name &amp;&amp; {name: name}) }; console.log(person); // { name: &#39;Peter&#39; } . Whether object person has property name or not is based on the value of variable name. We can also switch between properties conditionally: . let name = null; let person = { ...(name ? {name: name} : {anonymous: true}) }; console.log(person); // { anonymous: true } name = &#39;Peter&#39;; person = { ...(name ? {name: name} : {anonymous: true}) }; console.log(person); // { name: &#39;Peter&#39; } .",
            "url": "https://graysonliu.github.io/blog/javascript/node.js/webpack/react/2020/10/23/some-random-stuff-on-javascript.html",
            "relUrl": "/javascript/node.js/webpack/react/2020/10/23/some-random-stuff-on-javascript.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Awesome Github Actions",
            "content": "Deploy Github Pages with Github Actions . Just weeks ago, I set up this blog with Jekyll on Github Pages. However, I switched to fastpages because of Jupyter Notebook support. The swithcing isn’t that difficult, but something caught my attention during the process, and that is Github Actions. We have talked about the idea that only pushing source files to the Github repo and let Github Pages generate static files by its own. But Github Pages only supports Jekyll and limited number of plugins, and that is where Github Actions can help. . Basically, Github Actions is like a remote server that can execute some predefined commands. To use Github Actions to deploy our Github Pages, we still need to generate static files in our repo, but the generation can be done automatically by Github Actions, and generated files can be hosted in another branch other than master. Therefore, we can host all source files in the master branch, and every time we push commits to the master branch (e.g. new posts or modifications of layouts), the predefined Github Actions workflow is triggered and it will automatically generate all necessary static files in another branch. We just need to change the repo’s setting that decides which branch Github Pages site is built from. . Take my homepage repo as an example. I build it using React and Webpack from scratch. Webpack will generate static files in a folder called dist. This dist folder does not have to be in the master branch, but all files inside it should be the content of another branch called gh-pages, and the Github Pages is built from this branch. . There are some Github Actions in the marketplace to help us achieve that for Github Pages. I chose the one that fastpages uses, which is peaceiris/actions-gh-pages. The usage is actually pretty straightforward, the entire workflow is as follows: . name: build on: push: branches: - master jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 - name: Setup Node uses: actions/setup-node@v2.1.0 with: node-version: &#39;12.x&#39; - name: Cache dependencies uses: actions/cache@v2 with: path: ~/.npm key: ${{ runner.os }}-node-${{ hashFiles(&#39;**/package-lock.json&#39;) }} restore-keys: | ${{ runner.os }}-node- - run: npm ci - run: npm run build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./dist . At first, we give a name to the workflow, which is build. Then, we specify what will trigger this workflow: . on: push: branches: - master . That is, every time we push commits to the master branch, this workflow will be executed. Now, we define all commands in this workflow. We first specify the operating system, checkout the repo and setup Node.js of the version we want. The following action actions/cache@v2 is to fetch all cached dependencies using key, or create the cache of all dependencies if the cache does not exist, so that the workflow does not need to download all dependencies again next time. In Linux, npm downloads all packages to ~/.npm, which is the folder we need to cache. The key of the cache is named after the OS and the hashing result of package-lock.json, which is reasonable since these two will decide the packages that npm downloads. . Now, we run bash command npm ci to download all dependencies (if one dependency exists in cache, npm will just use the alreay downloaded package, while new dependencies will be downloaded from the Internet). Then, we run npm run build to generate all static files. In my package.json, npm run build will use Webpack to achieve that: . { ... &quot;scripts&quot;: { &quot;start&quot;: &quot;webpack-dev-server --mode development&quot;, &quot;build&quot;: &quot;webpack --mode production&quot; } ... } . Webpack generate all files in the /dist folder, and we should deploy all content in this folder to the branch gh-pages. That is excatly what peaceiris/actions-gh-pages does at last. As for ${{ secrets.GITHUB_TOKEN }}, this is an automactically generated token by Github for authentication in the workflow, since this action needs to be authorized to write into our repository. We will find more usages of secrets in the next part. . Periodically Spotify Playlists Updating with Github Actions . I once wrote a Python script that can update my Spotify top 200 playlists with data from SpotifyCharts. To update playlists periodically, this script will be executed everyday on my own server. However, with Github Actions, we can schedule a workflow that runs the script at any time. The repo is at graysonliu/spotify-top-200-playlist-generator. We use this workflow to update the playlists daily. At the beginning, apart from push-triggering, this workflow is also triggered at 00:00, 06:00, 12:00 and 18:00 every day: . on: push: branches: - master schedule: # * is a special character in YAML so you have to quote this string - cron: &#39;0 0,6,12,18 * * *&#39; . Similar to the previous Node.js project, we also need to checkout the repo, specify the version of operating system and Python, cache and install all dependencies that pip downloads: . - uses: actions/checkout@v2 - name: Set up Python 3.6 uses: actions/setup-python@v2 with: python-version: 3.6 - uses: actions/cache@v2 with: path: ~/.cache/pip key: ${{ runner.os }}-pip-${{ hashFiles(&#39;**/requirements.txt&#39;) }} restore-keys: | ${{ runner.os }}-pip- - name: Install dependencies run: | python -m pip install --upgrade pip if [ -f requirements.txt ]; then pip install -r requirements.txt; fi . Then, we execute the Python script to update our playlists. When we run this script locally, we can set client id and client secret of our Spotify app as environment variables in .env, and their values can be obtained in Python script by simply reading environment variables. However, sensitive information like client id and client secret should not be exposed, meaning that we should not host this .env file on Github. Therefore, we add them as secrets at Github, and set them as environment variables in the workflow (actually, client id is not sensitive at all, you can just put it in the config.yml). . Modifying playlists in Spotify needs authentication, and that requires a browser. However, since Github Actions works in a headless environment, it is impossible to use a browser for authentication. Our strategy is, we first authenticate the app locally, which will give us a .cache file that saves tokens and can be used for authentication for a long period of time. We create secret AUTH_CACHE that saves the content of .cache at Github and also set it as a environment variable in the runtime. In the workflow, when we execute the Python script, we fetch this environment variable and create a .cache file using this secret. This newly created .cache file will be used for authentication. . I tried to create .cache by echo command in the workflow directly: . run: echo ${{ secrets.SPOTIFY_AUTH_CACHE }} &gt;&gt; .cache . This will not work because AUTH_CACHE is a JSON string that has quotation marks inside it, but echo somehow omits those quotation marks. So, we as well use Python to achieve that: . auth_cache = os.getenv(&#39;AUTH_CACHE&#39;) if auth_cache: with open(&#39;.cache&#39;, &#39;w&#39;) as f: f.write(auth_cache) . Another thing is, tokens in .cache could be refreshed in the runtime. If we do not update AUTH_CACHE to keep up with the content of .cache, tokens saved in AUTH_CACHE could be expired. Therefore, after the playlists are updated, we have to write the content of .cache back to secret AUTH_CACHE in case file .cache changes (though it seems it never changes according to my experience). We have to use Github APIs to update secrets in a workflow, but that needs authentication from Github. The default ${{ secrets.GITHUB_TOKEN }} does not have the permission to write secrets. For that purpose, we need a token with special scopes of permissions. First, we create a personal access token named TOKEN_WRITE_SECRETS with scopes as follows: . . Copy the generated token, and add it to secrets. We also name this secret TOKEN_WRITE_SECRETS. . In total, we should have four secrets now: . . After the updating is finished, we write the content of .cache back to AUTH_CACHE in the Python script. To write a secret, we should encrypt its content using the repo’s public key first. This public key can also be acquired by Github APIs (see this): . # update secret AUTH_CACHE in case that content in .cache is changed during runtime (e.g. token is refreshed) secret_name = &#39;AUTH_CACHE&#39; # we need authentication to write secrets # default GITHUB_TOKEN does not have the permission to write secrets, we need to create a personal access token # we also need to set this personal access token as an environment variable token_write_secrets = os.getenv(&#39;TOKEN_WRITE_SECRETS&#39;) # these are default environment variables in Github Actions github_api_url = os.getenv(&#39;GITHUB_API_URL&#39;) github_repo = os.getenv(&#39;GITHUB_REPOSITORY&#39;) github_actor = os.getenv(&#39;GITHUB_ACTOR&#39;) # for authentication from requests.auth import HTTPBasicAuth auth = HTTPBasicAuth(github_actor, token_write_secrets) headers = {&#39;Accept&#39;: &#39;application/vnd.github.v3+json&#39;} # Get the public key to encrypt secrets # reference: https://docs.github.com/en/free-pro-team@latest/rest/reference/actions#get-a-repository-public-key r = requests.get(f&#39;{github_api_url}/repos/{github_repo}/actions/secrets/public-key&#39;, headers=headers, auth=auth) public_key = r.json()[&#39;key&#39;] key_id = r.json()[&#39;key_id&#39;] . Then, we invoke the Github API that modifies secrets (see this): . # reference: https://docs.github.com/en/free-pro-team@latest/rest/reference/actions#create-or-update-a-repository-secret from base64 import b64encode from nacl import encoding, public def encrypt(public_key: str, secret_value: str) -&gt; str: &quot;&quot;&quot;Encrypt a Unicode string using the public key.&quot;&quot;&quot; public_key = public.PublicKey(public_key.encode(&quot;utf-8&quot;), encoding.Base64Encoder()) sealed_box = public.SealedBox(public_key) encrypted = sealed_box.encrypt(secret_value.encode(&quot;utf-8&quot;)) return b64encode(encrypted).decode(&quot;utf-8&quot;) with open(&#39;.cache&#39;, &#39;r&#39;) as f: encrypted_value = encrypt(public_key, f.read()) data = {&#39;encrypted_value&#39;: encrypted_value, &#39;key_id&#39;: key_id} r = requests.put(f&#39;{github_api_url}/repos/{github_repo}/actions/secrets/{secret_name}&#39;, headers=headers, json=data, auth=auth) if r.ok: print(f&#39;Secret {secret_name} updated.&#39;) . Run the entire Python script in the workflow with four secrets we defined before: . - name: Set environment variables with secrets and update playlists env: CLIENT_ID: ${{ secrets.CLIENT_ID }} CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }} AUTH_CACHE: ${{ secrets.AUTH_CACHE }} TOKEN_WRITE_SECRETS: ${{ secrets.TOKEN_WRITE_SECRETS }} run: python generate_top_200_playlist.py . Now, this workflow will update Spotify playlists at scheduled times every day. It uses AUTH_CACHE for authentication for Spotify, and uses a personal access token TOKEN_WRITE_SECRETS for authentication for Github APIs that help us to write the content of .cache back to secret AUTH_CACHE. . At last, we remove .cache since its content has already been saved in secret AUTH_CACHE: . - name: Delete local authorization cache file run: rm -f .cache . This is mainly for security reasons, though it might not be necessary since this file was created in the workflow and its lifecycle should end in the current workflow run. .",
            "url": "https://graysonliu.github.io/blog/github/node.js/python/2020/10/14/awesome-github-actions.html",
            "relUrl": "/github/node.js/python/2020/10/14/awesome-github-actions.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Lazy Evaluation & Late Binding: Unexpected Behavior of Iterables in Python",
            "content": "Glossary . iterable in Python: . An object capable of returning its members one at a time. . Anything that can be used in a for loop, such as list, tuple, str, dict, set and iterators. We can convert an iterable to an iterator, which is actually what for statement does, by passing it to the built-in function iter(). . iterator in Python:&gt; An object representing a stream of data. It must have the __next__() method (or can be passed to the built-in function next()), such as generators, map objects. . The Problem . Recently, I&#39;ve been practicing on Leetcode. In problem 373. Find K Pairs with Smallest Sums, I implemented a solution inspired by discussions in the community using heapq.merge in Python. . The main idea of heapq.merge is like a multi-way merge sort on many already-sorted iterables. The difference is that heapq.merge uses a heap for ordering. Heap is just a binary tree in Python, and it is implemented with an array. heapq.merge initially collects the first element of each input iterable to form a list, then it heapify the list, transforming it to a heap. The first element of heap—h[0], is always the smallest in the heap (supposing it is a min-heap). heap.merge just pop h[0] and puts it into the result iterator, and then push the following element of h[0] from the same input iterable into the heap (more details at source code). We can also use heap as a priority queue. . The implemention is as follows: . from typing import List import heapq, itertools def kSmallestPairs(nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]: # we have m*n pairs, imagine a m*n matrix rows = (([u, v] for v in nums2) for u in nums1) # since both nums1 and nums2 are ascending # each row is already sorted based on the sum of u and v # regular slicing cannot be applied to generator since &#39;generator&#39; object does not have __getitem__ # we should use itertools.islice return list(itertools.islice(heapq.merge(*rows, key=sum), k)) nums1 = [1, 7, 11] nums2 = [2, 4, 6] print(kSmallestPairs(nums1, nums2, k=9)) . [[11, 2], [11, 2], [11, 2], [11, 4], [11, 4], [11, 4], [11, 6], [11, 6], [11, 6]] . rows is a generator of generators. We expect k pairs of smallest sums as the output, which should be: . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . But the output is not what we expected. Is something wrong with the generator? Try following code: . rows = (([u, v] for v in nums2) for u in nums1) for row in rows: for element in row: print(element, end=&#39; &#39;) . [1, 2] [1, 4] [1, 6] [7, 2] [7, 4] [7, 6] [11, 2] [11, 4] [11, 6] . Surprisingly, the output is what we expect. . Lazy Evaluation . We know that generators in Python are lazy evaluation. An element in a generator is only evaluated when we iterate to it. Most other objects in Python are eager evaluation: . j = 1 L = [j for _ in range(4)] # list G = (j for _ in range(4)) # generator j = 2 print(L) print(list(G)) . [1, 1, 1, 1] [2, 2, 2, 2] . In above snippet, list L is evaluated when j=1, while generator G is not evaluated until we call list(G), at that time j=2. . Back to the generator rows, if we make it a list of generators instead of a generator of generators: . rows = [([u, v] for v in nums2) for u in nums1] for row in rows: for element in row: print(element, end=&#39; &#39;) . [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] . The output is different from when rows is a generator, but it is similar to the output of heapq.merge. The only difference is we did not sort the list as in heapq.merge. . If we think about the evaluation mechanism, when we create rows as a list, since list is eager evaluation, u should be 11 after that line is executed. That is why when we iterate those three generators later, the first elements of all pairs are 11. When rows is a generator, since generators are lazy evaluation, the value of u only changes after we finish iterating each row. . Back to heapq.merge, which is actually heapq.merge(*iterables, key=None, reverse=False). The first parameter iterables becomes a tuple after it receives input iterables. We can easily check this by following snippet: . rows = (([u, v] for v in nums2) for u in nums1) def func(*iterables): print(type(iterables)) for it in iterables: for pair in it: print(pair, end=&#39; &#39;) func(*rows) . &lt;class &#39;tuple&#39;&gt; [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] [11, 2] [11, 4] [11, 6] . The output is the same as when rows is a list even if we pass rows as a generator. Because it is converted to a tuple in the function, and tuple is also eager evaluation just like list. . However, even if heapq.merge does not convert rows to a tuple, we still cannot expect correct output. We mentioned before that heapq.merge initially collects the first element of each input iterable, considering following code: . rows = (([u, v] for v in nums2) for u in nums1) it1 = next(rows) print(next(it1)) # [1, 2] it2 = next(rows) print(next(it1)) # [7, 4], we expect [1, 4] print(next(it2)) # [7, 2] it3 = next(rows) print(next(it1)) # [11, 6], we expect [1, 6] print(next(it2)) # [11, 4], we expect [7, 4] print(next(it3)) # [11, 2] . [1, 2] [7, 4] [7, 2] [11, 6] [11, 4] [11, 2] . We find that every time we call next(rows), the value of u changes. Thus, even if heapq.merge dose not convert rows to a tuple, we can only guarantee correct pairs for the first element in each generator. After that, the first value of all subsequent pairs will be the same. . Solution . The correct way to solve the above problem is to make rows a map object: . rows = map(lambda u: ([u, v] for v in nums2), nums1) print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . This map object is also converted to a tuple when passing to heapq.merge, however, it gives us correct result. map objects are iterators, using lazy evaluation just like generators. The difference here is that this uses a function to create three generators. That u here is a parameter of the lambda function, not a single variable that holds the value of an element in nums1, and this function will be called three times with different values passed in. Therefore, this actually has nothing to do with map. It really depends on what u is. If we define rows as follows, we can also get the correct output: . rows = ((lambda u: ([u, v] for v in nums2))(k) for k in nums1) print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . rows = [(lambda u: ([u, v] for v in nums2))(k) for k in nums1] print(list(heapq.merge(*rows, key=sum))) . [[1, 2], [1, 4], [1, 6], [7, 2], [7, 4], [7, 6], [11, 2], [11, 4], [11, 6]] . We replaced u with k, which holds the value of the element currently being iterated in nums1, to make the structure more clear. However, this is not necessary since even if we use u to iterate nums1, in the lambda function, u will be overwritten by the parameter u of the function. . To better understand how this u affects the output, we can also use lambda expressions to write the equivalents of previous definitions of rows that give us incorrect results: . rows = (([u, v] for v in nums2) for u in nums1) # is equivalent to rows = ((lambda: ([u, v] for v in nums2))() for u in nums1) rows = [([u, v] for v in nums2) for u in nums1] # is equivalent to rows = [(lambda: ([u, v] for v in nums2))() for u in nums1] . Late Binding . I also want to mention late binding here because it usually leads to unexpected behavior in Python, though not specifically limited to iterables. According to Wikipedia: . Late binding, dynamic binding, or dynamic linkage is a computer programming mechanism in which the method being called upon an object or the function being called with arguments is looked up by name at runtime. . There is a good article talking about late binding in Python. We will use some snippets to figure out how late binding works. . funcs = [lambda: u for u in range(5)] for func in funcs: print(func(), end=&#39; &#39;) . 4 4 4 4 4 . The output again is a repeat of the last element of range(5). We defined funcs, which is a list of 5 functions. Each function does not accept any parameters and returns u, which holds the value of the element being iterated in range(5). These functions are not called until we enter the following for loop and use func(). . Late binding means that values of variables used in a function are looked up at the time the function is called. When we call those five functions in funcs, they will all look up the value of u, which becomes 4 after we define funcs, because funcs, which is a list, use eager evaluation. However, if we define funcs as generators, the output becomes what we expect: . funcs = (lambda: u for u in range(5)) for func in funcs: print(func(), end=&#39; &#39;) . 0 1 2 3 4 . This is obvious since generators use lazy evaluation. The value of u changes along the for loop. . Another example of late binding without iterables: . i = 1 def func(): return i i = 2 print(func()) . 2 .",
            "url": "https://graysonliu.github.io/blog/coding/python/iterator/lazy-evaluation/late-binding/leetcode/heap/2020/09/22/lazy-evaluation-late-binding-unexpected-behavior-of-iterables-in-python.html",
            "relUrl": "/coding/python/iterator/lazy-evaluation/late-binding/leetcode/heap/2020/09/22/lazy-evaluation-late-binding-unexpected-behavior-of-iterables-in-python.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Using a Forked Jekyll Theme for Github Pages",
            "content": "Switch to Jekyll . I have abandoned blogging for a very long time. I used to build my blog on Hexo and hosted it on Github Pages. A major drawback of this solution is that we have to first generate all static files locally, and then push all those files to the remote repository. As for Jekyll, since it is officially supported by Github Pages, we only need to push sources to the remote repository. Github Pages will generate the static site automatically, and those static files are not a part of our repository, making our repo much tidier. . There is a great guide on blogging with Github Pages and Jekyll, but this solution also has a pain point. Although Jekyll has plenty of plugins developed by the community, Github Pages only supports some specific ones, let alone most supported plugins are of an older version without newest features. If we want to beautify or add features to the blog, we have to modify corresponding templates by ourselves. Of course, we can add those modified templates to our Github Pages repo directly, but what if we want to use them as a separate theme like a Ruby gem. If this new theme is a fork of an existing theme, we might also want to update it from the upstream. Although Github Pages only supports specific Jekyll plugins, it could utilize any Jekyll theme that is hosted on Github. Therefore, we need to create our own Jekyll theme and host it on Github, then specify it as the the theme that should be used by Github Pages. . Configuration . I forked the minima theme and plan to develop my own theme based on it. Give the forked repo a new name, better to be the name you want for this new theme, e.g. minima-graysonliu. Then, in *.gemspec, simply change the value of spec.name to minima-graysonliu. We now have a Github-hosted Jekyll theme called minima-graysonliu. To use this theme, navigate to our Github Pages repo, in _config.yml, delete the following line: . theme: ... . and add a new line: . remote_theme: graysonliu/minima-graysonliu . The value of the remote_theme is just the name of your new theme repo hosted on Github, and we are done. . Other changes are needed if we want to modify and test our new theme locally. We first clone both the blog (Github Pages) repo and the theme repo to our local disk. In Gemfile of the blog repo, change . # This is the default theme for new Jekyll sites. You may change this to anything you like. gem &quot;minima&quot; . to . # This is the default theme for new Jekyll sites. You may change this to anything you like. gem &quot;minima-graysonliu&quot;, :path =&gt; &quot;/LOCAL_PATH_TO_THEME_REPO/minima-graysonliu&quot; . Update with: . bundle update . Even if _config.yml in the blog repo does not have the key theme (because we replaced it with remote_theme), since we specify minima-graysonliu as the default theme, Jekyll will use it when we run the server locally. . If we need to keep our forked new theme updated with the original theme, just set the original repo as its upstream and sync the fork regularly. .",
            "url": "https://graysonliu.github.io/blog/jekyll/blog/github/2020/09/21/using-a-forked-jekyll-theme-for-github-pages.html",
            "relUrl": "/jekyll/blog/github/2020/09/21/using-a-forked-jekyll-theme-for-github-pages.html",
            "date": " • Sep 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Visit my website. .",
          "url": "https://graysonliu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://graysonliu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}